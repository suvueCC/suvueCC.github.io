---
layout: post
title:  "ELK Stack7.3.0日志收集平台（四）"
categories: ELK-Stack平台
tags: elk 日志收集
author: Zk1an
---

* content
{:toc}
> 本篇内容主要讲：
>
> - kafka集群安装配置
> - 轻量级日志采集组件Filebeat的安装使用

## 一、环境准备与安装

确保linux服务器上已经安装好jdk环境，没安装的[点这里安装](https://zhaoke1an.github.io/2020/10/15/CentOS%E5%8D%B8%E8%BD%BDopenJDK%E5%AE%89%E8%A3%85OracleJDK/)

### 1.1、下载

kafka官网下载地址:https://kafka.apache.org/downloads，当前最新版本如下，Scala 2.12指的是Scala版本号（kafka是Scala语音编写的），2.6.0是kafka版本。

![202011292116rfGkNn](../uPic/%202020%2011%2029%2021%2016rfGkNn.jpg)



压缩包下载后上传到服务器自定义目录，我的是/usr/local/elkstack/目录下，解压压缩包，后重名为kafka方便后续操作。

### 1.2、解压缩

```shell
[root@host elkstack]# tar -xzvf kafka_2.12-2.6.0.tgz 
```

为了方便后续操作，我将解压缩后的文件重命名为kafka  

```shell
[root@host elkstack]# mv kafka_2.12-2.6.0 kafka
```

## 二、配置文件修改

​         当前下载的kafka程序里自带Zookeeper，可以直接使用其自带的Zookeeper建立集群，也可以单独使用Zookeeper安装文件建立集群。***本次采用自带Zookeeper***，自带的Zookeeper程序脚本与配置文件名与原生Zookeeper稍有不同。kafka自带的Zookeeper程序在bin目录下的zookeeper-server-start.sh脚本进行启动，zookeeper-server-stop.sh脚本进行停止。另外Zookeeper的配制文件在路径config/zookeeper.properties，如果有需要可以修改其中的参数。

**首先强调一点**，kafka的日志目录和zookeeper数据目录，这两项默认放在tmp目录，而tmp目录中内容会随重启而丢失,所以我们遇到的时候最好自定义一个路径。

### 2.1、zookeeper配置文件

进入kafka/config目录下，参考如下修改zookeeper.properties文件

```shell
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# the directory where the snapshot is stored.
# zookeeper数据目录
dataDir=/usr/local/elkstack/kafka/data/kfkzookeeper
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
# 注释掉这个
#maxClientCnxns=0
 
# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
# admin.serverPort=8080
 
#设置连接参数，添加如下配置
tickTime=2000
#为zk的基本时间单元，毫秒
initLimit=10
#Leader-Follower初始通信时限 tickTime*10
syncLimit=5
#Leader-Follower同步通信时限 tickTime*5
 
#设置broker Id的服务地址，以下三个分别是我集群三台服务器ip
server.1=172.10.132.95:2888:3888
server.2=172.10.132.96:2888:3888
server.3=172.10.132.97:2888:3888
```

### 2.2、zookeeper配置myid文件

三台服务器都要在其zookeeper数据目录dataDir下创建一个myid文件，文件内只需填入上述配置文件中broker id的值，作为集群识别标识。以其中一台172.10.132.95服务器为例：

```shell
[root@host ~]# cd /usr/local/elkstack/kafka/data/kfkzookeeper
[root@host kfkzookeeper]# echo 1 > myid
```

### 2.3、kafka配置文件

进入kafka/config目录下，参考如下修改server.properties文件

```shell
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
 
# see kafka.server.KafkaConfig for additional details and defaults
 
############################# Server Basics #############################
 
# The id of the broker. This must be set to a unique integer for each broker.
# broker.id是kafka broker的编号，集群里每个broker的id需不同,且必须为int整数
broker.id=1
 
# 选择启用删除主题功能，默认false
delete.topic.enable=true
 
############################# Socket Server Settings #############################
 
# listeners是监听地址，需要提供外网服务的话，要设置本地的IP地址
# The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
 
# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092
 
# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
 
# The number of threads that the server uses for receiving requests from the network and sending responses to the network
# 服务器用来接受请求或者发送响应的线程数
num.network.threads=3
 
# The number of threads that the server uses for processing requests, which may include disk I/O
# 服务器用来处理请求的线程数，可能包括磁盘IO
num.io.threads=8
 
# The send buffer (SO_SNDBUF) used by the socket server
# 套接字服务器使用的发送缓冲区大小
socket.send.buffer.bytes=102400
 
# The receive buffer (SO_RCVBUF) used by the socket server
# 套接字服务器使用的接收缓冲区大小
socket.receive.buffer.bytes=102400
 
# The maximum size of a request that the socket server will accept (protection against OOM)
# 单个请求最大能接收的数据量
socket.request.max.bytes=104857600
 
 
############################# Log Basics #############################
 
# A comma separated list of directories under which to store log files
# log.dirs是日志(消息数据)目录
log.dirs=/usr/local/elkstack/kafka/data/kafka
 
# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
# num.partitions 为新建Topic的默认Partition数量，partition数量提升，一定程度上可以提升并发性
# 每个主题的日志分区的默认数量。更多的分区允许更大的并行操作，但是它会导致节点产生更多的文件
num.partitions=3
 
# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
# 每个数据目录用于在启动时恢复日志并在关闭时刷新的线程数,用于在启动时日志恢复，并在关闭时刷新。
# 对于数据目录位于RAID阵列中的安装，建议增大此值。
num.recovery.threads.per.data.dir=1
 
############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
# 内部topic配置
# 内部__consumer_offsets和__transaction_state两个topic，分组元数据的复制因子，为了保证可用性，在生产上建议设置大于1。
# default.replication.factor为kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务，是在自动创建topic时的默认副本数，可以设置为3
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3
 
############################# Log Flush Policy #############################
 
# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.
 
# The number of messages to accept before forcing a flush of data to disk
# 在强制刷新数据到磁盘之前允许接收消息的数量
#log.flush.interval.messages=10000
 
# The maximum amount of time a message can sit in a log before we force a flush
# 在强制刷新之前，消息可以在日志中停留的最长时间
#log.flush.interval.ms=1000
 
############################# Log Retention Policy #############################
 
# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.
 
# The minimum age of a log file to be eligible for deletion due to age
# 一个日志的最小存活时间，可以被删除
log.retention.hours=168
 
# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
# 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。
#log.retention.bytes=1073741824
 
# The maximum size of a log segment file. When this size is reached a new log segment will be created.
# 每一个日志段大小的最大值。当到达这个大小时，会生成一个新的片段。
log.segment.bytes=1073741824
 
# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
# 检查日志段的时间间隔，看是否可以根据保留策略删除它们
log.retention.check.interval.ms=300000
 
############################# Zookeeper #############################
 
# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
# 设置Zookeeper集群地址，我是在同一个服务器上搭建了kafka和Zookeeper，所以填的本地地址
zookeeper.connect=172.10.132.95:2181,172.10.132.96:2181,172.10.132.97:2181
 
# Timeout in ms for connecting to zookeeper
# 连接到Zookeeper的超时时间
zookeeper.connection.timeout.ms=18000
 
 
############################# Group Coordinator Settings #############################
 
# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0
```

## 三、启动集群服务

### 3.1、启动Zookeeper服务

三台机器依次执行以下命令：

```shell
[root@host kafka]# bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
```

这里的-daemon参数，可以在后台启动Zookeeper，不必打印启动日志到控制台，下同，输出的信息在保存在执行目录的logs/zookeeper.out文件中。

## 3.2、启动kafka服务

```shell
[root@host kafka]# bin/kafka-server-start.sh -daemon config/server.properties

```

查看启动日志，进入kafka下logs目录  

```shell
[root@host-192-168-11-21 logs]# cat server.log
```

## 四、常用命令

### 4.1、创建topic

```shell
[root@host kafka]# bin/kafka-topics.sh --create --zookeeper 172.10.132.95:2181 --topic test
```

### 4.2、查看topic列表

```shell
[root@host kafka]# bin/kafka-topics.sh --list --zookeeper 172.10.132.95:2181
```

### 4.3、查看topic详情

```shell
[root@host-192-168-11-21 kafka]# bin/kafka-topics.sh --zookeeper 172.10.132.95:2181 --describe  --topic test
```

### 4.3、创建生产者，在一台服务器

```shell
[root@host kafka]# bin/kafka-console-producer.sh --broker-list 172.10.132.95:9092 --topic test
```

### 4.4、创建消费者，在另一台服务器

```shell
[root@host kafka]# bin/kafka-console-consumer.sh --bootstrap-server 172.10.132.96:9092 --topic test
```

或者从头消费  

```shell
[root@host-192-168-11-23 kafka]# bin/kafka-console-consumer.sh --bootstrap-server 172.10.132.96:9092 --topic test --from-beginning
```

### 4.5、删除topic

```shell
[root@host kafka]# bin/kafka-topics.sh --zookeeper 172.10.132.95:2181 --delete  --topic test
```

**kafka集群到这里就搭建完成了！**

## 五、轻量级日志采集工具Filebeat的安装使用

### 5.1、下载Filebeat安装包

```shell
curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.3.0-linux-x86_64.tar.gz
```

### 5.2、解压安装包

```shell
tar -zxvf filebeat-7.3.0-linux-x86_64.tar.gz
```

### 5.3、配置文件filebeat.yml修改

这里以采集Linux的access日志为例简单说明

```yaml
filebeat.inputs:
- type: log
  enabled: true
  paths:
# paths为需要采集的日志的路径，可配置多个（每行以'-'开头，需注意格式，也可使用"*.log"通配符）
    - /usr/local/app/nginx/logs/access.log
  fields:
# fields为自定义的属性，最后可传递到kibana上进行分类筛选使用
    app: app01
    app_ip: 10.16.32.241
    app_port: 443
    kafka_topic: app-dev-ngxaccesslog
# tail_files表示是否从文件开头读取，历史日志的话一定是全部都要读取的，所以这里配置false，再次重启的话也不用进行更改，filebaet会
# 对每个文件标记上次读取的位置，所以tail_files属性只对第一次启动有效
  tail_files: false
# multiline为多行匹配配置
# multiline.pattern为多行匹配的正则，系统会认为一行日志就是一条日志，这往往不是我们想要的结果，
# 所以我们要根据实际情况编写正则，去匹配业务上的一条日志，比如Nginx是以时间开头，代表一条日志的开始，所以我们就要根据Nginx的时间格式，去写正则
# 切记：正则只支持R2,也就是简单的匹配规则，我的理解是这样的
  multiline.pattern: '^\d{1,3}\.\d{1,3}.\d{1,3}.\d{1,3}'
# 下面这两个配置可以不用动
  multiline.negate: true
  multiline.match: after

# 下面和上面是一样的，去采集别的类型日志
- type: log
  enabled: true
  paths:
    - /usr/local/nginx/logs/error.log
  fields:
    app: app01
    app_ip: 10.16.32.241
    app_port: 443
    kafka_topic: app-dev-ngxerrorlog
  tail_files: false
  multiline.pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}'
  multiline.negate: true
  multiline.match: after

# 输出到kafka
output.kafka:
  enabled: true
  hosts: ["172.10.132.95:9092","172.10.132.96:9092","172.10.132.97:9092"]
  topic: '%{[fields][kafka_topic]}'
```

### 5.4、启动filebeat

```shell
[root@host kafka]# su elkstack
[root@host kafka]# ./filebeat -e -c filebeat.yml
```

### 5.5、后台启动filebeat

```shell
nohup ./filebeat -e -c filebeat.yml -d "*" > filebeat_server.log 2>&1 &
```



### 5.6、停止filebeat

```shell
kill -9 进程号
```

### 5.7、 常见问题

- 正常情况下，同一个文件的日志filebeat只会去采集一次，倘若我们遇到需要多次从头采集日志的情况时，可以删除${filebeat_home}/data/registry整个目录，然后重启filebeat。
- 当日志文件非常大的时候，filebeat去采集时，自己本身就会打印大量的日志（上述的filebeat_server.log），从而会把客户业务系统的磁盘打满，解决方案就是当调试ok时，将日志抛弃，即"> /dev/null"